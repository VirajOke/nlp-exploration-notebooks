{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP toolkit part 1: ETL\n",
    "## Load text data from HTML, TXT, PDF and DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T19:42:37.349853Z",
     "start_time": "2022-07-18T19:42:21.808882Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcsR31sa9py6",
    "outputId": "9be27f5f-0dba-4b86-a3aa-e4503ec640e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install python-docx -q\n",
    "! pip install pyPDF2 -q\n",
    "! pip install html2text -q\n",
    "! pip install contractions -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tSkw0D0Y72C"
   },
   "source": [
    "### Import packages for ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.808Z"
    },
    "id": "6P3DaVAhleLR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import re\n",
    "import glob \n",
    "import docx\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20FNQhGEoJDg",
    "outputId": "aa6b670d-d68f-4b1e-e9e5-95c49420b141"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDWMcxzoZSyq"
   },
   "source": [
    "### get_textfile_paths() helper function\n",
    "Returns a list of absolute paths to all pdf, html, doc and txt files within a folder.\n",
    "\n",
    "If `folder_path` is not supplied as an argument, it is set to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.812Z"
    },
    "id": "huAxiBITSMkw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def get_textfile_paths(folder_path=None):\n",
    "    if not folder_path or len(folder_path) < 1:\n",
    "        folder_path = os.getcwd()\n",
    "    input_files = []\n",
    "    data_types = ['/*.doc*','/*.pdf','/*.html','/*.txt']\n",
    "    for i in data_types:\n",
    "        temp_input_files = glob.glob(folder_path + i)\n",
    "        input_files.extend(temp_input_files)\n",
    "    return input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppjOeAc_q8VS"
   },
   "source": [
    "### doc_to_text() helper function\n",
    "\n",
    "For each file path in supplied `file_paths` list:\n",
    "1. Check the file extensions\n",
    "2. Use appropriate transform for file extension to extract plain text\n",
    "3. Append extracted text to dictionary {filename: extracted_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.815Z"
    },
    "id": "Rjw6sTZ128wH"
   },
   "outputs": [],
   "source": [
    "def doc_to_text(file_paths, clean=False):\n",
    "    out_dict = dict()\n",
    "    for doc in file_paths:\n",
    "        try:\n",
    "            extracted_text = ''\n",
    "\n",
    "            file_extension = pathlib.Path(doc).suffix\n",
    "            filename = os.path.basename(doc)\n",
    "\n",
    "            if file_extension == '.docx':\n",
    "                word_doc = docx.Document(doc) \n",
    "                for words in word_doc.paragraphs:\n",
    "                    extracted_text += words.text \n",
    "\n",
    "            elif file_extension == '.pdf':\n",
    "                reader = PdfReader(doc)\n",
    "                for page in reader.pages:\n",
    "                    extracted_text += page.extract_text()\n",
    "\n",
    "            elif file_extension == '.html':\n",
    "                with open(doc, 'r') as f:\n",
    "                    h = html2text.HTML2Text()\n",
    "                    h.ignore_links= True\n",
    "                    html_data = f.read()\n",
    "                extracted_text = h.handle(html_data)\n",
    "\n",
    "            elif file_extension == '.txt':\n",
    "                with open(doc, 'r') as f:\n",
    "                    extracted_text = f.read() \n",
    "\n",
    "            # Data CLeaning \n",
    "            if clean:\n",
    "                # remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "                extracted_text = re.sub(r\"http\\S+\", \"\", str(extracted_text))\n",
    "                # https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "                extracted_text = BeautifulSoup(extracted_text, 'lxml').get_text()\n",
    "                extracted_text = contractions.fix(extracted_text)\n",
    "                # remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "                extracted_text = re.sub(\"\\S*\\d\\S*\", \"\", extracted_text).strip()\n",
    "                # remove special character: https://stackoverflow.com/a/5843547/4084039\n",
    "                extracted_text = re.sub('[^A-Za-z]+', ' ', extracted_text)\n",
    "                # remove all the words which often seen common from the sentences\n",
    "                # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "                # dict_text = ' '.join(e.lower() for e in dict_text.split() if e.lower() not in stopwords)\n",
    "\n",
    "            out_dict[filename] = extracted_text\n",
    "        except:\n",
    "            print('Error decoding', doc)\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3kIjUk3oAWv"
   },
   "source": [
    "*text_from_dir()* will call *get_textfile_paths()*, then pass the textfile paths to *doc_to_text()*, which transforms the files and returns the final dictionary of textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.818Z"
    },
    "id": "8-Mz6Og75SY_"
   },
   "outputs": [],
   "source": [
    "def text_from_dir(dir_path=None, clean=False):\n",
    "    file_paths = get_textfile_paths(dir_path)\n",
    "    out_dict = doc_to_text(file_paths, clean=clean)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnRr5tzQpcD6"
   },
   "source": [
    "**input_folder** stores the path of the input folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.821Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cn6EaXxzUOwp",
    "outputId": "db2d337b-4cd9-42e1-ed88-94e48f089b62"
   },
   "outputs": [],
   "source": [
    "\"\"\" \"/content/drive/MyDrive/Colab_Notebooks1/Practice/SSC_GCA\" this is my local system path \"\"\"\n",
    "\n",
    "# input_folder = str(input(\"Please enter the folder path: \"))\n",
    "# data_cleaning = bool(input(\"Please enter 'True' if you want to clean the data & 'False' otherwise: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.825Z"
    },
    "id": "3pkOo-geWB4J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_folder = None\n",
    "data_cleaning = True\n",
    "\n",
    "final_data = text_from_dir(input_folder, data_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.827Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzpgvgVkWHnY",
    "outputId": "0165f7be-fd03-407a-abfd-354f697455a2"
   },
   "outputs": [],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyRn-ZplcZa5"
   },
   "source": [
    "Below is the list of keys with documents from the input folder (file names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.830Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "injuLrQIcQHI",
    "outputId": "a3d8e199-ba49-468f-8809-0c3f5ed19812"
   },
   "outputs": [],
   "source": [
    "keys = list(final_data)\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNYuIbvQX-CY"
   },
   "source": [
    "* The dictionary stores the file names and respective textual data. The values can be retrived using keys.\n",
    "* e.g. `final_data['Artemis_NASA.html']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.832Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "mi_Vd_Lv7EZ1",
    "outputId": "c1648d41-635f-4d4c-b648-2a04530b80ce"
   },
   "outputs": [],
   "source": [
    "first_key = list(final_data)[0]\n",
    "print(first_key)\n",
    "print(final_data[first_key][:2000], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIDfnlVpqrwy"
   },
   "source": [
    "Below is the final output dict that contains key-value pairs for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.834Z"
    },
    "id": "SNnkEERpL7QM"
   },
   "outputs": [],
   "source": [
    "# final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R38LERyF2Flb"
   },
   "source": [
    "The block below pickles the final dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-18T19:42:21.836Z"
    },
    "id": "rqGzZJ0CMP77"
   },
   "outputs": [],
   "source": [
    "# filename= '/content/drive/MyDrive/Colab_Notebooks1/Practice/SSC_GCA/pickled_data'\n",
    "# outfile= open(filename, 'wb')\n",
    "# pickle.dump(final_data, outfile)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Wxc-B_32fq7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ETL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
